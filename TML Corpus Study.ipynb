{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import bs4\n",
    "import cltk\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize.punkt import PunktLanguageVars\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import ward, dendrogram   \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from cltk.stop.latin.stops import STOPS_LIST\n",
    "\n",
    "import itertools\n",
    "\n",
    "stop_words = STOPS_LIST.extend(['quod', 'vel', 'sunt', 'hoc', 'vero', 'sit', 'sol', 'que', 'esse'])\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CLTK:Pulling latest 'latin_text_latin_library' from 'https://github.com/cltk/latin_text_latin_library.git'.\n",
      "INFO:CLTK:Pulling latest 'latin_pos_lemmata_cltk' from 'https://github.com/cltk/latin_pos_lemmata_cltk.git'.\n",
      "INFO:CLTK:Pulling latest 'latin_models_cltk' from 'https://github.com/cltk/latin_models_cltk.git'.\n"
     ]
    }
   ],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "\n",
    "corpus_importer = CorpusImporter('latin')\n",
    "corpus_importer.import_corpus('latin_text_latin_library')\n",
    "corpus_importer.import_corpus('latin_pos_lemmata_cltk')\n",
    "corpus_importer.import_corpus('latin_models_cltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This corpus comes from the Thesaurus Musicarum Latinarum.\n",
    "\n",
    "corpus_files = glob.glob('corpus/html/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soups = [bs4.BeautifulSoup(open(file), 'lxml') for file in corpus_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_soup(soup):\n",
    "    \"Takes a bs4 BeautifulSoup object and returns a dict with document content and metadata.\"\n",
    "    corpus_item = {}\n",
    "    \n",
    "    header = soup.find('p')\n",
    "\n",
    "    original = \" \".join([n for n in soup.findAll(text=True)])\n",
    "    \n",
    "    body = \"\".join([p.text for p in soup.findAll('p')[1:]])\n",
    "    body = remove_newlines(body)\n",
    "    body = remove_doublespace(body)\n",
    "    body = remove_editorial_apparatus(body)\n",
    "    body = join_hyphens(body)\n",
    "    \n",
    "    century_re = re.compile('.*www.chmtl.indiana.edu\\/tml\\/(\\w+\\W*\\w+)\\/.*')\n",
    "    \n",
    "    try:\n",
    "        comments = \"\".join(soup.findAll(text=lambda text:isinstance(text, bs4.Comment)))\n",
    "        century = century_re.match(comments).groups()[0]\n",
    "        corpus_item['century'] = century\n",
    "    except:\n",
    "        corpus_item['century'] = 'nd'\n",
    "\n",
    "    corpus_item['body'] = body.strip()\n",
    "    corpus_item['id'] = soup.title.text.split(' ')[0]\n",
    "    \n",
    "    match = re.search(r\"Author:\\s(.*)\", original)\n",
    "    \n",
    "    if match:\n",
    "        result = match.group(1)\n",
    "    else:\n",
    "        result = \"\"\n",
    "    \n",
    "    corpus_item['author'] = result\n",
    "    \n",
    "    match = re.search(r\"Title:\\s(.*)\", original)\n",
    "    \n",
    "    if match:\n",
    "        result = match.group(1)\n",
    "    else:\n",
    "        result = \"\"\n",
    "        \n",
    "    corpus_item['title'] = result\n",
    "    \n",
    "    return corpus_item\n",
    "\n",
    "def remove_editorial_apparatus(text):\n",
    "    \"Removes all text enclosed in square brackets.\"\n",
    "    pattern = re.compile('\\[.+?\\]')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "def remove_newlines(text):\n",
    "    \"Replaces newlines with spaces.\"\n",
    "    return text.replace('\\n', ' ')\n",
    "\n",
    "def remove_doublespace(text):\n",
    "    \"Removes any doublespaces.\"\n",
    "    return text.replace('  ', ' ')\n",
    "\n",
    "def join_hyphens(text):\n",
    "    \"Joins hyphens used at ends of lines.\"\n",
    "    return text.replace('-\\n', '')\n",
    "\n",
    "def my_tokenize(document):\n",
    "    \"Tokenizes a document, represented as a string using Punkt.\"\n",
    "    p = PunktLanguageVars()\n",
    "    tokens = p.word_tokenize(document)\n",
    "    tokens = [x.lower() for x in tokens if x not in (',', ';', '.', \"'\", '\"',':',')','(', '|' , '||' )]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for soup in soups:\n",
    "    try:\n",
    "        corpus_item = process_soup(soup)    \n",
    "        corpus.append(corpus_item)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write texts stripped of boilerplate for use externally \n",
    "\n",
    "import unidecode\n",
    "\n",
    "for item in corpus:\n",
    "    with open('./corpus/body/{}.txt'.format(item['id']), 'w') as f:\n",
    "        f.write(unidecode.unidecode(item['body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_id_info_map(corpus):\n",
    "    id_info_map = {}\n",
    "    for item in corpus:\n",
    "        item_copy = item.copy()\n",
    "        del item_copy['body']\n",
    "        id_info_map[item['id']] = item_copy\n",
    "    return id_info_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_info_map = prepare_id_info_map(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Aaron, Petrus',\n",
       " 'century': '16th',\n",
       " 'id': 'AARIH1',\n",
       " 'title': 'De institutione harmonica, liber primus'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_info_map['AARIH1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_stopwords(tokens):\n",
    "    \"Filters stopwords from list of tokens.\"\n",
    "    return [token for token in tokens if token not in STOPS_LIST]\n",
    "\n",
    "def filter_shortwords(tokens, short_size=2):\n",
    "    \"Filters tokens of size greater than short_size from list of tokens.\"\n",
    "    return [token for token in tokens if len(token) > short_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cltk.stem.lemma import LemmaReplacer\n",
    "\n",
    "lemmatizer = LemmaReplacer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prep(document):\n",
    "    \"A convenience function which applies a series of operations to a document represented in a string.\"\n",
    "    tokenized = my_tokenize(document)\n",
    "    stopped = filter_stopwords(tokenized)\n",
    "    shorted = filter_shortwords(stopped)\n",
    "    lemmatized = lemmatizer.lemmatize(shorted)\n",
    "    done = lemmatized\n",
    "    return done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [prep(doc['body']) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gensim.corpora.MmCorpus.serialize('tml.mm', bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_corpus = gensim.corpora.MmCorpus('tml.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(bow_corpus, normalize=True)\n",
    "tfidf_corpus = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi = gensim.models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi.print_topics(100)[60:90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focusing on a subset\n",
    "\n",
    "This is some code to restrict sample to a certain set of parameters\n",
    "\n",
    "TODO: Make a proper query interface for corpus entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_century = '17th'\n",
    "q_min_size = 50\n",
    "q_max_size = 10000000\n",
    "\n",
    "sample = [treatise for treatise in corpus\\\n",
    "          if treatise['century'] == q_century\\\n",
    "          and len(treatise['body']) < q_max_size\\\n",
    "          and len(treatise['body']) > q_min_size]\n",
    "\n",
    "bodys = [treatise['body'] for treatise in sample]\n",
    "ids = [treatise['id'] for treatise in sample]\n",
    "titles = [treatise.get('title', 'nt')[:15] for treatise in sample]\n",
    "authors = [treatise.get('author', 'na')[:15] for treatise in sample]\n",
    "\n",
    "len(sample) , len(ids), len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lengths = [len(treatise['body']) for treatise in sample]\n",
    "plt.hist(lengths, bins=12)\n",
    "plt.title('Histogram of text lengths (characters) in 13C treatises.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity - Ward clustering in tf-idf space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [doc['body'] for doc in corpus]\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=prep, stop_words=stop_words)\n",
    "tfs = tfidf.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bodys is the sample only from the 13th century \n",
    "\n",
    "tfs = tfidf.transform(bodys)\n",
    "tfs.shape\n",
    "\n",
    "dist = 1 - cosine_similarity(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linkage_matrix = ward(dist)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 15)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=authors);\n",
    "\n",
    "plt.tick_params(axis='x',          \n",
    "    which='both',      \n",
    "    bottom='off',      \n",
    "    top='off',         \n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text reuse - Viral Texts\n",
    "\n",
    "Reimplementation of the Viral Texts methodology\n",
    "\n",
    "viraltexts.org\n",
    "\n",
    "Note that this is done at the \"section\" level - will need to \"sectionize\" documents first.\n",
    "\n",
    "Part I. Identify document pair candidates\n",
    "\n",
    "1. Pick out a subset of all the texts for testing purposes\n",
    "2. Sectionalize\n",
    "3. ~~Make a hashmap id -> tokenized text~~\n",
    "4. Make a hashmap id -> shingled text\n",
    "5. For each n-gram, create a list of (d_i, p_i) - document ID and position ID\n",
    "6. Exclude singleton n-grams\n",
    "7. Extract candidate pairs\n",
    "8. Filter (e.g. suppress n-grams that generate more than some fixed number of pairs)\n",
    "9. Rank\n",
    "\n",
    "Part II. Local document alignment\n",
    "\n",
    "1. Do Smith-Waterman on top pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N = 5\n",
    "\n",
    "def zipngram(document_tokens,n=N):\n",
    "    return zip(*[document_tokens[i:] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_ngram_index(id_shingled_map):\n",
    "    ngram_index = defaultdict(list)\n",
    "    \n",
    "    for _id, shingled_rep in id_shingled_map.items():\n",
    "        for position, n_gram in enumerate(shingled_rep):\n",
    "            ngram_index[n_gram].append((_id, position))\n",
    "    \n",
    "    return ngram_index\n",
    "\n",
    "def filter_singletons(ngram_index):\n",
    "    # Naive implementation not advised in article\n",
    "    # TODO: Implement nice two-pass hash fxn. method\n",
    "    filtered_ngram_index = {}\n",
    "    \n",
    "    for ngram, locations in ngram_index.items():\n",
    "        if len(locations) > 1:\n",
    "            filtered_ngram_index[ngram] = locations\n",
    "    \n",
    "    return filtered_ngram_index\n",
    "\n",
    "def prepare_id_tokenized_map(corpus):\n",
    "    id_tokenized_map = {}\n",
    "    \n",
    "    for text in corpus:\n",
    "        id_tokenized_map[text['id']] = my_tokenize(text['body'])\n",
    "        \n",
    "    return id_tokenized_map\n",
    "\n",
    "def prepare_id_body_map(corpus):\n",
    "    id_body_map = {}\n",
    "    \n",
    "    for text in corpus:\n",
    "        id_body_map[text['id']] = text['body']\n",
    "            \n",
    "    return id_body_map \n",
    "\n",
    "def prepare_id_shingled_map(corpus): \n",
    "    id_shingled_map = {}\n",
    "    \n",
    "    for text in corpus:\n",
    "        tokenized_body = my_tokenize(text['body'])\n",
    "        id_shingled_map[text['id']] = [ngram for ngram in zipngram(tokenized_body, N)]\n",
    "    \n",
    "    return id_shingled_map\n",
    "\n",
    "def prepare_pair_ngrams_map(ngram_index):\n",
    "    pair_ngrams_map = defaultdict(list)\n",
    "    \n",
    "    for ngram, locations in ngram_index.items():\n",
    "        documents = list(set([location[0] for location in locations]))\n",
    "        for p in itertools.combinations(documents, 2):\n",
    "            if p[0][:3] != p[1][:3]:\n",
    "                pair_ngrams_map[p].append(ngram)\n",
    "    \n",
    "    return pair_ngrams_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'century', 'author', 'body', 'id'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i+n]\n",
    "    \n",
    "def sectionalize_corpus(corpus, section_length=50):\n",
    "    sectionalized_corpus = []\n",
    "    for item in corpus:\n",
    "        words = item['body'].split(' ')\n",
    "        for index, chunk in enumerate(chunks(words, section_length)):\n",
    "            old_id = item.get('id')\n",
    "            new_item = item.copy()\n",
    "            new_item['id'] = old_id + '_{}'.format(index)\n",
    "            new_item['body'] = ' '.join(chunk)\n",
    "            sectionalized_corpus.append(new_item)\n",
    "    return sectionalized_corpus        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = sectionalize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Jacobus Leodiensis',\n",
       " 'body': 'b mollem secundum ut de F; sed re per  durum secundum ut de G. Et consimili modo intelligendum est de distinctis vocibus in aliis positis clavibus. Ex his etiam videri potest quare in tribus primis litteris et in ultima sunt sole voces, quare in quarta due, in quinta due,',\n",
       " 'century': '14th',\n",
       " 'id': 'JACSM6B_100',\n",
       " 'title': 'Speculum musicae, Liber sextus'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60.4 ms, sys: 8.32 ms, total: 68.8 ms\n",
      "Wall time: 74.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time id_body_map = prepare_id_body_map(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 s, sys: 174 ms, total: 15.5 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%time id_tokenized_map = prepare_id_tokenized_map(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 s, sys: 396 ms, total: 17.6 s\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "%time id_shingled_map = prepare_id_shingled_map(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 s, sys: 14.5 s, total: 30.4 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%time idx = prepare_ngram_index(id_shingled_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.38 s, sys: 1.94 s, total: 4.32 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%time filtered_idx = filter_singletons(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(idx) > len(filtered_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing to search for candidate document pairs, let's go on an make a dict that has pairs of documents as its keys and a list of ngrams that each share is its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.59 s, sys: 1.06 s, total: 5.66 s\n",
      "Wall time: 7.91 s\n"
     ]
    }
   ],
   "source": [
    "%time pair_ngrams_map = prepare_pair_ngrams_map(filtered_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def p_n_map_ngrams_freq(pair_ngrams_map):\n",
    "    freqs = Counter()\n",
    "    for pair, ngrams in pair_ngrams_map.items():\n",
    "        freqs[pair] += len(ngrams)\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486598"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = p_n_map_ngrams_freq(pair_ngrams_map)\n",
    "len(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_to_align = [f[0] for f in freq.most_common(100000)]\n",
    "len(pairs_to_align)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do Smith-Waterman using multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import align\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "def best_alignment_by_id(id_tuple):\n",
    "    text_a = id_body_map[id_tuple[0]]\n",
    "    text_b = id_body_map[id_tuple[1]]\n",
    "    a = pairwise2.align.localms(text_a, text_b, 2, -1, -.5, -.1)\n",
    "    return (id_tuple, a[0])\n",
    "\n",
    "pool = multiprocessing.Pool(processes=2)\n",
    "pool_outputs = pool.map(best_alignment_by_id, pairs_to_align)\n",
    "pool.close() \n",
    "pool.join() \n",
    "\n",
    "print(len(pool_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is embarassingly slow. This seems legit:\n",
    "\n",
    "https://github.com/noporpoise/seq-align\n",
    "\n",
    "but it doesn't have a Python API (yet...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "CPU times: user 10.4 ms, sys: 122 ms, total: 133 ms\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def work(cmd):\n",
    "    return subprocess.call(cmd, shell=False)\n",
    "\n",
    "def invoke_seq_align(id_tuple):\n",
    "    text_a = id_body_map[id_tuple[0]]\n",
    "    text_b = id_body_map[id_tuple[1]]\n",
    "    args = ['/home/eamonn/Downloads/seq-align/bin/smith_waterman', text_a, text_b]\n",
    "    return work(args)\n",
    "\n",
    "count = multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool(processes=count)\n",
    "print(pool.map(invoke_seq_align, pairs_to_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = -numpy.ones((256, 256)) + 2 * numpy.identity(256)\n",
    "S = S.astype(numpy.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "CPU times: user 394 ms, sys: 568 ms, total: 961 ms\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "f = open('out.put', 'w')\n",
    "\n",
    "def invoke_align(id_tuple):\n",
    "    text_a = id_body_map[id_tuple[0]]\n",
    "    text_b = id_body_map[id_tuple[1]]\n",
    "    s1 = align.string_to_alignment(text_a)\n",
    "    s2 = align.string_to_alignment(text_b)\n",
    "    (s, a1, a2) = align.align(s1, s2, -2, -2, S, local=True)\n",
    "    a1s = align.alignment_to_string(a1)\n",
    "    a2s = align.alignment_to_string(a2)\n",
    "    f.write('{}\\t{}\\t{}\\t{}\\n'.format(id_tuple, a1s, a2s, s))\n",
    "    \n",
    "    return 0\n",
    "\n",
    "count = multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool(processes=2)\n",
    "print(len(pool.map(invoke_align, pairs_to_align)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extra stuff\n",
    "\n",
    "\n",
    "### Create a borrowing network\n",
    "\n",
    "Create a digraph with weighted edges from the pair-ngram-map where the weight is given by the number of shared n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def p_n_map_to_weighted_edges(pair_ngrams_map, min_ngrams=50, max_ngrams=5000):\n",
    "    edges = []\n",
    "    for pair, ngrams in pair_ngrams_map.items():\n",
    "        if min_ngrams < len(ngrams) < max_ngrams:\n",
    "            edges.append(\"{},{},{}\\n\".format(pair[0], pair[1], len(ngrams)))\n",
    "    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edges = p_n_map_to_weighted_edges(pair_ngrams_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_edges_to_file(edges, file=open('graph.txt', 'w')):\n",
    "    file.write('source,target,weight\\n')\n",
    "    file.writelines(edges)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_edges_to_file(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text reuse - Naive n-gram recurrence heatmap\n",
    "\n",
    "This was the original effort to visualize text reuse. It would be nice to have this interactive, with a hover system that showed:\n",
    "\n",
    "1. The ngram that is being shared\n",
    "2. Sources for each n-gram (reference, link)\n",
    "3. Other statistics about the n-gram\n",
    "\n",
    "Note that the tokenizer here is really dumb; and we don't do any lemmatizing etc. This is because we're just interested here in literal text reuse.\n",
    "\n",
    "Could reuse the indexes generated above to acheive the same ends.\n",
    "\n",
    "1. Convert each document to a list of n-grams (say n=3)\n",
    "2. Convert the remainder of the corpus to a list of n-grams (same n), and that into a Counter\n",
    "3. For each document, iterate through the list of n-grams\n",
    "4. If the n-gram is appears in the rest of the corpus, make a note of that and how many times it appears\n",
    "5. End up with a \"heatmap\" of reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "import itertools\n",
    "\n",
    "N = 3\n",
    "\n",
    "sample = random.sample(corpus, 25)\n",
    "bodys = [item['body'] for item in sample]\n",
    "ids = [item['id'] for item in sample]\n",
    "\n",
    "corpus_tokens_list = [my_tokenize(document) for document in bodys]\n",
    "\n",
    "def zipngram(document_tokens,n=N):\n",
    "    return zip(*[document_tokens[i:] for i in range(n)])\n",
    "\n",
    "def generate_slice(index, document, sample):\n",
    "    # Document being studied for text reuse\n",
    "    document = document\n",
    "    # Make a list of the other documents\n",
    "    others = [d for j, d in enumerate(sample) if j != index]\n",
    "\n",
    "    # Tokenize\n",
    "    document_tokens = my_tokenize(document)\n",
    "    others_tokens_list = corpus_tokens_list\n",
    "\n",
    "    # Get n-grams\n",
    "    document_ngrams_generator = zipngram(document_tokens, N)\n",
    "    document_ngrams = [x for x in document_ngrams_generator]\n",
    "    others_ngrams_list = [zipngram(other_tokens, N) for other_tokens in others_tokens_list]\n",
    "\n",
    "    # Create counter implemented with defaultdict for remainders\n",
    "    flat = itertools.chain(*others_ngrams_list)\n",
    "    o_counter = Counter(flat)\n",
    "\n",
    "    heatslice = [o_counter.get(ngram, 0) for ngram in document_ngrams]\n",
    "    return heatslice, document_ngrams\n",
    "\n",
    "def construct_heatmap():\n",
    "    for index, document in enumerate(bodys):\n",
    "        a_slice = generate_slice(index, document, bodys)\n",
    "        heatmap.append(a_slice[0])\n",
    "        documents_ngrams.append(a_slice[1])\n",
    "\n",
    "def plot_heatmap2():\n",
    "    # Use subplots and contourf to create heatmap bands (per document)\n",
    "    f, axarr = plt.subplots(len(heatmap), figsize=(15, 20))\n",
    "    for index, data in enumerate(heatmap):\n",
    "        h = np.array(data)\n",
    "        x = np.empty([2,h.shape[0]])\n",
    "        x[:,:] = h\n",
    "        axarr[index].contourf(x)\n",
    "    f.show()\n",
    "\n",
    "def plot_heatmap():\n",
    "    # First go which just puts everything on the one heatmap\n",
    "    length = len(sorted(heatmap,key=len, reverse=True)[0])\n",
    "    hm = np.array([xi+[0]*(length-len(xi)) for xi in heatmap])\n",
    "    fig, ax = plt.subplots(figsize=(15, 20))\n",
    "    ax.set_yticks(np.arange(hm.shape[0])+0.5, minor=False)\n",
    "    ax.set_yticklabels(ids)\n",
    "    ax.pcolormesh(hm)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heatmap = []\n",
    "documents_ngrams = []\n",
    "\n",
    "%matplotlib inline\n",
    "construct_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_heatmap2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## More stuff about the indexes compiled earlier \n",
    "\n",
    "Let's have a look inside this index, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ngram_index_freq(index, blacklist=[]):\n",
    "    freqs = Counter()\n",
    "    for ngram, locations in index.items():\n",
    "        if not bool(set(ngram) & set(blacklist)):\n",
    "            freqs[ngram] += len(locations)\n",
    "    return freqs\n",
    "\n",
    "freq = ngram_index_freq(filtered_idx)\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of scale segments... We could do a couple of things about this, I suppose. Let's find out the most common tokens in the dataset and exclude any ngrams from this list which contain a token that appears close to the top of that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_all_tokens(id_tokenized_map):\n",
    "    all_tokens = []\n",
    "    for _id, tokens in id_tokenized_map.items():\n",
    "        all_tokens.extend(tokens)\n",
    "    return Counter(all_tokens)\n",
    "\n",
    "token_counter = count_all_tokens(id_tokenized_map)\n",
    "\n",
    "def top_n_tokens(id_tokenized_map, n):\n",
    "    token_counter = count_all_tokens(id_tokenized_map)\n",
    "    return [t[0] for t in token_counter.most_common(n)]\n",
    "\n",
    "top_100_tokens = top_n_tokens(id_tokenized_map, 100)\n",
    "\n",
    "freq = ngram_index_freq(filtered_idx, blacklist=top_100_tokens)\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit better this time. Let's check out the provenance of the phrase:\n",
    "\n",
    "> acuti soni gravisque mixtura suaviter\n",
    "\n",
    "which appears 26 times in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locations = filtered_idx[('acuti', 'soni', 'gravisque', 'mixtura', 'suaviter')]\n",
    "for _id, position in locations:\n",
    "    info = id_info_map[_id]\n",
    "    print(_id, info['author'][:15], info['title'][:15], position, info['century'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about this for a research question?\n",
    "\n",
    "> What treatises does Descartes allude to/paraphrase in his *Compendium*?\n",
    "\n",
    "1. Use id_shingled_map to get the shingled version of Descartes' text, 'DESCOM'\n",
    "2. Look up each n-gram in the filtered_idx for concordances\n",
    "3. Filter out disinteresting ones\n",
    "4. Print out the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_id = 'DESCOM'\n",
    "query_shingled = [ngram for ngram in id_shingled_map[query_id]]\n",
    "\n",
    "len(query_shingled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = []\n",
    "\n",
    "for query_ngram in query_shingled:\n",
    "    locations = filtered_idx.get(query_ngram, None)\n",
    "    if locations is not None:\n",
    "        response.append((query_ngram, locations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
